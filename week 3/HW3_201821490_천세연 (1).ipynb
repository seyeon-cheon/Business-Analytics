{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝\n",
    "\n",
    "#### - 텍스트 마이닝의 이해\n",
    "\n",
    "#### - 텍스트 마이닝 방법론\n",
    "\n",
    "#### - 텍스트 마이닝 문제점\n",
    "\n",
    "#### - 문제 해결을 위한 방안\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝이란?\n",
    "\n",
    "    - The process ofderiving high-quality information from text\n",
    "    - Turn unstructured text into structured data,\n",
    "    - 일정한 길이 (sparse or dense) 의 vector로 변환\n",
    "    \n",
    "    - and use the structured data in various tasks such as \n",
    "    - text classification, clustering, sentiment analysis, document summarization, translation, prediction, etc.\n",
    "    - 변환된 vector에 머신러닝 (딥러닝) 기법을 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 방법\n",
    "\n",
    "   - NLP\n",
    "    - Tokenize, stemming, lemmatize\n",
    "    - Chunking\n",
    "    - BOW, TFIDF – sparse representation\n",
    "    \n",
    " \n",
    " \n",
    " - 머신러닝(딥러닝)\n",
    "    - Naïve Bayes, Logistic egression, Decision tree, SVM\n",
    "    - Embedding(Word2Vec, Doc2Vec) – dense representation\n",
    "    - RNN(LSTM), Attention, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 적용분야\n",
    "\n",
    "- Document classification\n",
    "    - Sentiment analysis, classification\n",
    "- Document generation\n",
    "    - Q&A, summarization, translation\n",
    "- Keyword extraction\n",
    "    - tagging/annotation\n",
    "- Topic modeling\n",
    "    - LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝 도구 - 파이썬\n",
    "\n",
    "    - NLTK : 가장 많이 알려진 NLP 라이브러리\n",
    "    - Scikit Learn : 머신러닝 라이브러리, 기본적인 NLP, 다양한 텍스트 마이닝 관련 도구 지원\n",
    "    - Gensim : Word2Vec으로 유명\n",
    "    - Keras : RNN, seq2seq 등 딥러닝 위주의 라이브러리 제공\n",
    "    \n",
    "\n",
    "## 텍스트 마이닝 기본 도구\n",
    "\n",
    "### 목적 : document, sentence 등을 sparse vector로 변환\n",
    "\n",
    "#### Tokenize : 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "\n",
    "    - Document를 Sentence의 집합으로 분리\n",
    "    - Sentence를 Word의 집합으로 분리\n",
    "    - 의미 없는 문자 등을 걸러 냄\n",
    "    - 영어는 space를 기준으로 비교적 쉽게 tokenize가 가능하지만, 한글은 구조상 형태소 분석이 필요하다    \n",
    "    \n",
    "#### Text normalization : 최소 단위를 표준화\n",
    "\n",
    "    - 동일한 의미의 단어가 다른 형태를 갖는 것을 보완한다\n",
    "        - 다른 형태의 단어들을 통일시켜 표준단어로 변환\n",
    "    - Stemming (어간 추출)\n",
    "    - Lemmatization (표제어 추출) : 어휘 데이터 베이스를 이용한 Wordnet Iemmatizer가 많이 쓰임\n",
    "    \n",
    "#### POS-tagging : 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "\n",
    "    - 토큰화와 정규화 작업을 통해 나누어진 형태소(의미를 가지는 최소단위)에 대해 품사를 결정하여 할당하는 작업\n",
    "    - 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "\n",
    "#### Chunking : POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "    \n",
    "    - Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사 구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미\n",
    "    - 즉, 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정\n",
    "    \n",
    "#### BOW\n",
    "    - Vector Space Model: 문서를 bag of words로 표현\n",
    "    - 모든 문서에 한번 이상 나타난 단어들에 대해 유(1)/무(0) 로 문서를 표현\n",
    "    - 순서는 무시\n",
    "    - count vector: 단어의 유/무 대신 단어가 문서에 나타난 횟수로 표현\n",
    "        - count가 weight로 작용\n",
    "        \n",
    "#### TFIDF \n",
    "    \n",
    "    - 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "    - tf(d, t): 문서 d에 단어 t가 나타난 횟수, count vector와 동일, 로그스케일 등 다양한 변형이 있음\n",
    "    - df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수\n",
    "    - idf(t): df(t)의 역수를 바로 쓸 수도 있으나, 여러가지 이유로 로그스케일과 스무딩을 적용한 공식을 사용,log(n/(1+df(t)), n은 전체 문서 수\n",
    "    \n",
    "#### Text Classification with BOW/TFIDF\n",
    "\n",
    "- Naïve Bayes\n",
    "- Logistic regression\n",
    "    - Ridge regression\n",
    "    - Lasso regression\n",
    "- Decision tree\n",
    "    - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 마이닝의 문제\n",
    "\n",
    "    - 차원의 저주\n",
    "        : 더 많은 데이터를 가져와서 학습시킨다\n",
    "        \n",
    "    - 단어 빈도의 불균형\n",
    "        : 멱법칙 (빈도가 높은 단어를 삭제. 심한 경우 50% 삭제)\n",
    "        \n",
    "    - Loss of sequence information\n",
    "        : n-gram, deep learning \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제의 해결 방안\n",
    "\n",
    "- 차원의 저주를 해결하기 위한 노력\n",
    "\n",
    "        - Feature selection\n",
    "           - Manual, Regularization(Lasso)\n",
    "        - Feature extraction\n",
    "           - PCA, LSA(SVD)   \n",
    "        - Embedding\n",
    "           - Word embedding, Document embedding\n",
    "        - Deep Learning\n",
    "           - RBM, Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling \n",
    "\n",
    "- 토픽은 주제를 의미하는 용어로 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 주제로부터 특정 단어들이 파생되어 나올 확률분포가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산\n",
    "\n",
    "        - θ: 문서들이 각 주제들에 속할 확률분포\n",
    "        - 디리클레분포의 매개변수인 <알파>에 의해 결정\n",
    "        - N: 특정 문서에 속한 단어의 집합\n",
    "        - M: 전체 문서의 집합\n",
    "        - z: 문서 내의 단어들이 주제들에 속할 확률분포\n",
    "        - θ에 의한 다항분포로 선택\n",
    "        - β: 각 주제가 특정 단어를 생성할 확률을 나타내는 확률분포\n",
    "        - 결국 z와 β에 의해 실제 문서들의 단어분포인 w가 결정\n",
    "        - w만이 실제로 문서들을 통해 주어진 분포이고 나머지는 모두 잠재변수\n",
    " \n",
    "- LDA 알고리즘에서는 주어진 문서와 토픽들의 사전확률 분포인 α와 토픽 내에서 단어의 사전확률분포인 β의 파라미터 값을 활용해 반복적인 시뮬레이션을 통해 z와 θ를추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "- 단어에 대한 vector의 dimension reduction이 목표\n",
    "\n",
    "- Word embedding:\n",
    "\n",
    "        -one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "            - \n",
    "        - 변환된 vector를 이용하여 학습\n",
    "        - 최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    "        - 학습목적 관점에서의 단어의 의미를 내포\n",
    "        \n",
    "        \n",
    "- BOW와는 다른 관점의 문서 표현\n",
    "     - document: 제한된 maxlen 개의 word sequence (앞이나 뒤를 잘라냄)\n",
    "     - word: one-hot-vector에서 저차원(reduced_dim)으로 embedding된 dense vector\n",
    "     - 즉 document는 (maxlen, reduced_dim)의 2차원 행렬로 표현\n",
    "     - 단순한 분류모형 (sequence 무시)\n",
    "    - (maxlen, reduced_dim) 차원의 document를 maxlen*reduced_dim 차원으로 펼쳐서 분류모형에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo\n",
    "\n",
    "- 사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론. \n",
    "- 이전의 대표적인 임베딩 기법인 Word2Vec이나 GloVe 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는것을 반영하지 못하는 것에 비해, ELMo는 이러한 문맥을반영하기 위해 개발된 워드 임베딩 기법. \n",
    "- 문맥의 파악을 위해 biLSTM으로 학습된 모형을 이용\n",
    "\n",
    "### Document Embedding\n",
    "- Word2Vec은 word에 대해 dense vector를 생성하지만, document vector는 여전히 sparse\n",
    "- Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성\n",
    "- 이 dense vector를 이용해 매칭, 분류 등의 작업 수행\n",
    "\n",
    "### RBM (Restricted Boltzmann Machine)\n",
    "\n",
    "- 사전학습 목적으로 개발\n",
    "            \n",
    "      - G. Hinton에 의해 제안\n",
    "      - 차원을 변경하면서 원래의 정보량 유지가 목적\n",
    "      - 정보량을 물리학의 에너지 함수로 표현\n",
    "- Deep NN의 vanishing gradient 문제 해결을 위해 제안\n",
    "    \n",
    "      - batch normalization, Dropout, ReLU 등의 기법으로 인해 문제가 해결되면서 지금은 많이 쓰이지 않음\n",
    "\n",
    "- 사전학습을 통한 차원 축소에 사용 가능\n",
    "\n",
    "### Autoencoder\n",
    "\n",
    "- RBM과 유사한 개념\n",
    "    \n",
    "       - encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "       \n",
    "- 작동방식은 PCA와 유사\n",
    "\n",
    "        -데이터에 내재된 일정한구조 – 연관성을 추출\n",
    "        \n",
    "###  N-gram\n",
    "-  문맥(context)를 파악하기 위한 전통적 방법\n",
    "- bi-gram, tri-gram, …\n",
    "- 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리\n",
    "\n",
    "### 딥러닝 – RNN\n",
    "\n",
    "- 문장을 단어들의 sequence 혹은 series로 처리\n",
    "- 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    "- 그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안\n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "- RNN의 문제\n",
    "     - 문장이 길수록 층이 깊은 형태를 갖게 됨  경사가 소실되는 문제 발생 -> 앞부분의 단어 정보가 학습되지 않음\n",
    "\n",
    "- LSTM: 직통 통로를 만들어 RNN의 문제를 해결\n",
    "\n",
    "### 합성곱 신경망, CNN \n",
    "\n",
    "- CNN은 원래 이미지 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음. \n",
    "\n",
    "- CNN은 합성곱층(conolution layer)와 풀링층(pooling)으로 구성되며, 합성곱층은 2차원 이미지에서 특정 영역의 특징을 추출하는 역할을 하는데, 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음.\n",
    "\n",
    "\n",
    "### Sequence to sequence\n",
    "\n",
    "- 지금까지는 입력은 sequence, 출력은 하나의 값인 경우가 일반적이었으나, 번역, chat-bot, summarize등은 출력도 sequence가 되어야 함\n",
    "- encoder, decoder의 구조를 가짐\n",
    "\n",
    "### Attention\n",
    "\n",
    "- 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "- 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "\n",
    "### Transformer (Self-attention)\n",
    "- 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "        \n",
    "        - 즉 입력 -> 출력으로의 attention 외에 입력 단어들 간의attention, 입력 + 출력 ->  출력으로의 attention을 추가\n",
    "\n",
    "- RNN이 사라지고 self-attention이 이를 대신\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations formTransformer)\n",
    "\n",
    "- 양방향 transformer 인코더를 사용\n",
    "- transformer에 기반한 OpenAI GPT와의 차이\n",
    "- transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
